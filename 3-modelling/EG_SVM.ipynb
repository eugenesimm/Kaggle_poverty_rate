{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer, log_loss, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "CLEAN_DATA_DIR = \"../data/clean/\"\n",
    "RESULT_DATA_DIR = \"../data/model_result/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SVM on unfilled dataset\n",
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with na values are:  ['Q06', 'Q07', 'Q08', 'Q11', 'Q19']\n"
     ]
    }
   ],
   "source": [
    "svm_unfilled = pd.read_csv(os.path.join(CLEAN_DATA_DIR, \"TRAIN_MERGED_UNFILLED.csv\"))\n",
    "\n",
    "missing_columns = svm_unfilled.columns[svm_unfilled.isnull().any()].tolist()\n",
    "print( 'Columns with na values are: ', missing_columns)\n",
    "\n",
    "# Fill na values with 'missing'\n",
    "svm_unfilled['Q06'] = svm_unfilled['Q06'].fillna(-1)\n",
    "svm_unfilled['Q07'] = svm_unfilled['Q07'].fillna(-1)\n",
    "svm_unfilled['Q08'] = svm_unfilled['Q08'].fillna(-1)\n",
    "svm_unfilled['Q11'] = svm_unfilled['Q11'].fillna(-1)\n",
    "svm_unfilled['Q19'] = svm_unfilled['Q19'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After One-Hot Encoding:\n",
      "     psu_hh_idcode   hhid  subjectivePoverty_rating  q02  q03  q05  q09  q23  \\\n",
      "0           30_8_1   3008                         4    1    1   44    0    0   \n",
      "1          194_1_2  19401                         1    2    2   48    0    0   \n",
      "2          224_6_1  22406                         3    1    1   61    0    0   \n",
      "3         323_10_1  32310                         5    1    1   66    0    0   \n",
      "4         428_10_1  42810                         4    2    1   72    0    0   \n",
      "...            ...    ...                       ...  ...  ...  ...  ...  ...   \n",
      "5329       571_8_1  57108                         3    2    1   73    0    0   \n",
      "5330       601_5_1  60105                         4    1    1   60    0    0   \n",
      "5331       782_1_1  78201                         2    1    1   55    0    0   \n",
      "5332       606_3_1  60603                         5    1    1   53    0    1   \n",
      "5333       450_4_1  45004                         4    1    1   78    0    0   \n",
      "\n",
      "      Q01  Q03  ...  Q11_7.0  Q11_8.0  Q11_10.0  Q11_11.0  Q11_12.0  Q11_13.0  \\\n",
      "0       1    1  ...      0.0      0.0       0.0       0.0       0.0       1.0   \n",
      "1       1    1  ...      0.0      0.0       0.0       0.0       0.0       1.0   \n",
      "2       1    1  ...      0.0      0.0       0.0       0.0       0.0       1.0   \n",
      "3       1    1  ...      0.0      0.0       0.0       0.0       0.0       1.0   \n",
      "4       1    1  ...      0.0      0.0       0.0       0.0       0.0       0.0   \n",
      "...   ...  ...  ...      ...      ...       ...       ...       ...       ...   \n",
      "5329    1    1  ...      0.0      0.0       0.0       0.0       0.0       0.0   \n",
      "5330    1    1  ...      0.0      0.0       0.0       0.0       0.0       1.0   \n",
      "5331    1    1  ...      0.0      0.0       0.0       0.0       0.0       1.0   \n",
      "5332    1    1  ...      0.0      0.0       0.0       0.0       0.0       0.0   \n",
      "5333    1    1  ...      0.0      0.0       0.0       0.0       0.0       1.0   \n",
      "\n",
      "      Q11_14.0  Q19_-1.0  Q19_1.0  Q19_2.0  \n",
      "0          0.0       0.0      0.0      1.0  \n",
      "1          0.0       0.0      0.0      1.0  \n",
      "2          0.0       0.0      0.0      1.0  \n",
      "3          0.0       0.0      0.0      1.0  \n",
      "4          1.0       0.0      0.0      1.0  \n",
      "...        ...       ...      ...      ...  \n",
      "5329       0.0       0.0      0.0      1.0  \n",
      "5330       0.0       0.0      0.0      1.0  \n",
      "5331       0.0       0.0      0.0      1.0  \n",
      "5332       0.0       0.0      0.0      1.0  \n",
      "5333       0.0       0.0      0.0      1.0  \n",
      "\n",
      "[5334 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode categorical columns\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
    "encoded = encoder.fit_transform(svm_unfilled[missing_columns])\n",
    "\n",
    "\n",
    "# Convert to DataFrame and combine with numerical features\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(missing_columns))\n",
    "numerical_df = svm_unfilled.drop(columns=missing_columns)\n",
    "\n",
    "# Combine numerical and encoded categorical data\n",
    "processed_df = pd.concat([numerical_df, encoded_df], axis=1)\n",
    "\n",
    "print(\"\\nAfter One-Hot Encoding:\")\n",
    "print(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       4\n",
      "1       1\n",
      "2       3\n",
      "3       5\n",
      "4       4\n",
      "       ..\n",
      "5329    3\n",
      "5330    4\n",
      "5331    2\n",
      "5332    5\n",
      "5333    4\n",
      "Name: subjectivePoverty_rating, Length: 5334, dtype: int64\n",
      "(4267, 45) (1067, 45) (4267,) (1067,)\n"
     ]
    }
   ],
   "source": [
    "y = processed_df['subjectivePoverty_rating']\n",
    "print(y)\n",
    "\n",
    "feature_cols = list(processed_df.columns.difference(['psu_hh_idcode', 'hhid', 'subjectivePoverty_rating']))\n",
    "X = processed_df[feature_cols]\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_x)\n",
    "X_test_scaled = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyeprparameter tuning with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch CV\n",
    "param_grid = {\n",
    "    'C': [0.5, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "optimal_params = GridSearchCV(SVC(probability=True, random_state=42), param_grid, cv=5, scoring='neg_log_loss', verbose=2)\n",
    "optimal_params.fit(X_train_scaled, train_y)\n",
    "\n",
    "print(\"Best Parameters:\", optimal_params.best_params_)\n",
    "print(\"Best Log Loss:\", optimal_params.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "# Best Parameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
    "# Best Log Loss: -1.948786842738619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = optimal_params.cv_results_\n",
    "log_loss_scores = results['mean_test_score']  # Mean log loss (negative)\n",
    "hyperparameters = results['params'] \n",
    "results_df = pd.DataFrame(hyperparameters)\n",
    "results_df['Mean Log Loss'] = -log_loss_scores  # Convert back to positive (lower is better)\n",
    "\n",
    "# Display the results sorted by Log Loss\n",
    "results_df = results_df.sort_values(by='Mean Log Loss', ascending=True)\n",
    "results_df.head()\n",
    "\n",
    "#Save the results to csv file\n",
    "results_df.to_csv(os.path.join(RESULT_DATA_DIR, \"svm_unfillled.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9633481316666286"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = optimal_params.predict_proba(X_test_scaled)\n",
    "log_loss(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For my reference, predict with optimized parameters HARD CODED Version\n",
    "\n",
    "svm_model = SVC(C=10, gamma=0.01, kernel='rbf', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, train_y)\n",
    "pred = svm_model.predict_proba(X_test_scaled)\n",
    "log_loss(test_y, pred)\n",
    "#1.9642149345026059"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
