{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "CLEAN_DATA_DIR = \"../data/clean/\"\n",
    "\n",
    "train_A = pd.read_csv(os.path.join(CLEAN_DATA_DIR, \"TRAIN_A.csv\"))\n",
    "train_B = pd.read_csv(os.path.join(CLEAN_DATA_DIR, \"TRAIN_B.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "\n",
    "def generate_best_RF_model(train_A):\n",
    "    X = X_train = train_A.drop(columns=['psu_hh_idcode', 'subjectivePoverty_rating'], axis='columns')\n",
    "    y = train_A['subjectivePoverty_rating']\n",
    "\n",
    "    # Define the parameter grid\n",
    "    params = {\n",
    "        'n_estimators': [ 200, 500],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [4, 5],\n",
    "        'min_samples_split': [2, 5, 50],\n",
    "        'min_samples_leaf': [35, 42, 50],\n",
    "    }\n",
    "\n",
    "    # Create the scorer\n",
    "    # log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        param_grid=params,\n",
    "        scoring='neg_log_loss',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X, y)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    # Return the best model\n",
    "    return best_model\n",
    "\n",
    "# def predict_ratings_RF(model, data):\n",
    "#     test_input_x = data.drop(columns=['psu_hh_idcode'])\n",
    "\n",
    "#     id = data['psu_hh_idcode']\n",
    "#     y_val_pred_proba = model.predict_proba(test_input_x)\n",
    "\n",
    "#     column_names = [f\"subjective_poverty_{i}\" for i in range(1, 11)]\n",
    "#     probs = pd.DataFrame(y_val_pred_proba, columns=column_names)\n",
    "#     pred = pd.concat([id, probs], axis=1)\n",
    "\n",
    "#     return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "def generate_best_XGB_model(train_data):\n",
    "\n",
    "    X_train = train_data.drop(columns=['psu_hh_idcode', 'subjectivePoverty_rating'], axis='columns')\n",
    "    y_train = train_data['subjectivePoverty_rating'] - 1\n",
    "    param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.3, 0.5, 0.7],\n",
    "    'colsample_bytree': [0.4, 0.6, 0.8]\n",
    "    }\n",
    "\n",
    "    # Create the XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', random_state=101)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_log_loss',  # Use log loss as the evaluation metric\n",
    "        cv=5,                    \n",
    "        verbose=1,               \n",
    "        n_jobs=-1                \n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    #print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    #print(\"Best Log Loss Score:\", -grid_search.best_score_)\n",
    "\n",
    "    best_model_xgb = grid_search.best_estimator_\n",
    "    return best_model_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def predict_ratings_RF(model, train_B_X):\n",
    "    test_ids = train_B_X['psu_hh_idcode']\n",
    "    train_B_X = train_B_X.drop(columns=['psu_hh_idcode'])\n",
    "    preds_proba = model.predict_proba(train_B_X)\n",
    "    output_df = pd.DataFrame(preds_proba, columns=[f'subjective_poverty_{i+1}' for i in range(preds_proba.shape[1])])\n",
    "    output_df.insert(0, 'psu_hh_idcode', test_ids.values)  # Insert the ID column at the start\n",
    "    return output_df\n",
    "\n",
    "def predict_ratings_XGB(model, train_B_X):\n",
    "    test_ids = train_B_X['psu_hh_idcode']\n",
    "    train_B_X = train_B_X.drop(columns=['psu_hh_idcode'])\n",
    "    preds_proba = model.predict_proba(train_B_X)\n",
    "    output_df = pd.DataFrame(preds_proba, columns=[f'subjective_poverty_{i+1}' for i in range(preds_proba.shape[1])])\n",
    "    output_df.insert(0, 'psu_hh_idcode', test_ids.values)  # Insert the ID column at the start\n",
    "    return output_df\n",
    "\n",
    "def predict_ratings_SVM(model, train_B_X):\n",
    "    test_ids = train_B_X['psu_hh_idcode']\n",
    "    train_B_X = train_B_X.drop(columns=['psu_hh_idcode'])\n",
    "    preds_proba = model.predict_proba(train_B_X)\n",
    "\n",
    "    # Identify categorical columns\n",
    "    missing_columns = [col for col in train_B_X.columns if -1 in train_B_X[col].unique()]\n",
    "    \n",
    "    # One-hot encode categorical columns\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
    "    encoded = encoder.fit_transform(train_B_X[missing_columns])\n",
    "\n",
    "    # Convert to DataFrame and combine with numerical features\n",
    "    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(missing_columns))\n",
    "    numerical_df = train_B_X.drop(columns=missing_columns)\n",
    "\n",
    "    # Combine numerical and encoded categorical data\n",
    "    processed_train_B_X = pd.concat([numerical_df, encoded_df], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    train_B_X_scaled = scaler.transform(processed_train_B_X)\n",
    "    preds_prob = model.predict_proba(train_B_X_scaled)\n",
    "    output_df = output_df = pd.DataFrame(preds_proba, columns=[f'subjective_poverty_{i+1}' for i in range(preds_proba.shape[1])])\n",
    "    output_df.insert(0, 'psu_hh_idcode', test_ids.values)  # Insert the ID column at the start\n",
    "    return output_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "model_rf = generate_best_RF_model(train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['q02', 'q03', 'q05', 'q09', 'q23', 'Q01', 'Q03', 'Q06', 'Q07',\n",
       "       'Q08', 'Q11', 'Q19'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_B_X = train_B.drop(columns=['subjectivePoverty_rating'])\n",
    "P_RF = predict_ratings_RF(model_rf, train_B_X)\n",
    "P_RF.to_csv(os.path.join(\"../data/train_B_preds/train_B_preds_rf.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models/rf_trained_on_filled_A.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "from joblib import dump\n",
    "\n",
    "dump(model_rf, \"saved_models/rf_trained_on_filled_A.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
