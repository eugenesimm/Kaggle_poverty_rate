{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "CLEAN_DATA_DIR = \"../data/clean/\"\n",
    "\n",
    "train_A = pd.read_csv(os.path.join(CLEAN_DATA_DIR, \"TRAIN_A.csv\"))\n",
    "train_B = pd.read_csv(os.path.join(CLEAN_DATA_DIR, \"TRAIN_B.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "\n",
    "def generate_best_RF_model(train_A):\n",
    "    X = X_train = train_A.drop(columns=['psu_hh_idcode', 'subjectivePoverty_rating'], axis='columns')\n",
    "    y = train_A['subjectivePoverty_rating']\n",
    "\n",
    "    # Define the parameter grid\n",
    "    params = {\n",
    "        'n_estimators': [ 200, 500],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [4, 5],\n",
    "        'min_samples_split': [2, 5, 50],\n",
    "        'min_samples_leaf': [35, 42, 50],\n",
    "    }\n",
    "\n",
    "    # Create the scorer\n",
    "    # log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        param_grid=params,\n",
    "        scoring='neg_log_loss',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X, y)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    # Return the best model\n",
    "    return best_model\n",
    "\n",
    "# def predict_ratings_RF(model, data):\n",
    "#     test_input_x = data.drop(columns=['psu_hh_idcode'])\n",
    "\n",
    "#     id = data['psu_hh_idcode']\n",
    "#     y_val_pred_proba = model.predict_proba(test_input_x)\n",
    "\n",
    "#     column_names = [f\"subjective_poverty_{i}\" for i in range(1, 11)]\n",
    "#     probs = pd.DataFrame(y_val_pred_proba, columns=column_names)\n",
    "#     pred = pd.concat([id, probs], axis=1)\n",
    "\n",
    "#     return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "def generate_best_XGB_model(train_data):\n",
    "\n",
    "    X_train = train_data.drop(columns=['psu_hh_idcode', 'subjectivePoverty_rating'], axis='columns')\n",
    "    y_train = train_data['subjectivePoverty_rating'] - 1\n",
    "    param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.3, 0.5, 0.7],\n",
    "    'colsample_bytree': [0.4, 0.6, 0.8]\n",
    "    }\n",
    "\n",
    "    # Create the XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', random_state=101)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_log_loss',  # Use log loss as the evaluation metric\n",
    "        cv=5,                    \n",
    "        verbose=1,               \n",
    "        n_jobs=-1                \n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    #print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    #print(\"Best Log Loss Score:\", -grid_search.best_score_)\n",
    "\n",
    "    best_model_xgb = grid_search.best_estimator_\n",
    "    return best_model_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def predict_ratings_RF(model, train_B_X):\n",
    "    test_ids = train_B_X['psu_hh_idcode']\n",
    "    train_B_X = train_B_X.drop(columns=['psu_hh_idcode'])\n",
    "    preds_proba = model.predict_proba(train_B_X)\n",
    "    output_df = pd.DataFrame(preds_proba, columns=[f'subjective_poverty_{i+1}' for i in range(preds_proba.shape[1])])\n",
    "    output_df.insert(0, 'psu_hh_idcode', test_ids.values)  # Insert the ID column at the start\n",
    "    return output_df\n",
    "\n",
    "def predict_ratings_XGB(model, train_B_X):\n",
    "    test_ids = train_B_X['psu_hh_idcode']\n",
    "    train_B_X = train_B_X.drop(columns=['psu_hh_idcode'])\n",
    "    preds_proba = model.predict_proba(train_B_X)\n",
    "    output_df = pd.DataFrame(preds_proba, columns=[f'subjective_poverty_{i+1}' for i in range(preds_proba.shape[1])])\n",
    "    output_df.insert(0, 'psu_hh_idcode', test_ids.values)  # Insert the ID column at the start\n",
    "    return output_df\n",
    "\n",
    "def predict_ratings_SVM(model, train_B_X):\n",
    "    test_ids = train_B_X['psu_hh_idcode']\n",
    "    train_B_X = train_B_X.drop(columns=['psu_hh_idcode'])\n",
    "    preds_proba = model.predict_proba(train_B_X)\n",
    "\n",
    "    # Identify categorical columns\n",
    "    missing_columns = [col for col in train_B_X.columns if -1 in train_B_X[col].unique()]\n",
    "    \n",
    "    # One-hot encode categorical columns\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
    "    encoded = encoder.fit_transform(train_B_X[missing_columns])\n",
    "\n",
    "    # Convert to DataFrame and combine with numerical features\n",
    "    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(missing_columns))\n",
    "    numerical_df = train_B_X.drop(columns=missing_columns)\n",
    "\n",
    "    # Combine numerical and encoded categorical data\n",
    "    processed_train_B_X = pd.concat([numerical_df, encoded_df], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    train_B_X_scaled = scaler.transform(processed_train_B_X)\n",
    "    preds_prob = model.predict_proba(train_B_X_scaled)\n",
    "    output_df = output_df = pd.DataFrame(preds_proba, columns=[f'subjective_poverty_{i+1}' for i in range(preds_proba.shape[1])])\n",
    "    output_df.insert(0, 'psu_hh_idcode', test_ids.values)  # Insert the ID column at the start\n",
    "    return output_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "model_rf = generate_best_RF_model(train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q01', 'Q03', 'Q06', 'Q07', 'Q08', 'Q11', 'Q19', 'q02', 'q03', 'q05', 'q09', 'q23']\n",
      "Index(['psu_hh_idcode', 'q02', 'q03', 'q05', 'q09', 'q23', 'Q01', 'Q03', 'Q06',\n",
      "       'Q07', 'Q08', 'Q11', 'Q19'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "feature_cols = list(train_A.columns.difference(['psu_hh_idcode', 'subjectivePoverty_rating']))\n",
    "print(feature_cols)\n",
    "\n",
    "print(train_B.drop(columns=['subjectivePoverty_rating']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_B_X \u001b[39m=\u001b[39m train_B\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39msubjectivePoverty_rating\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m P_RF \u001b[39m=\u001b[39m predict_ratings_RF(model_rf, train_B_X)\n\u001b[1;32m      3\u001b[0m P_RF\u001b[39m.\u001b[39mto_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m../data/train_B_preds/train_B_preds_rf.csv\u001b[39m\u001b[39m\"\u001b[39m), index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36mpredict_ratings_RF\u001b[0;34m(model, train_B_X)\u001b[0m\n\u001b[1;32m      4\u001b[0m test_ids \u001b[39m=\u001b[39m train_B_X[\u001b[39m'\u001b[39m\u001b[39mpsu_hh_idcode\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m train_B_X \u001b[39m=\u001b[39m train_B_X\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpsu_hh_idcode\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m preds_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(train_B_X)\n\u001b[1;32m      7\u001b[0m output_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(preds_proba, columns\u001b[39m=\u001b[39m[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msubjective_poverty_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(preds_proba\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])])\n\u001b[1;32m      8\u001b[0m output_df\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpsu_hh_idcode\u001b[39m\u001b[39m'\u001b[39m, test_ids\u001b[39m.\u001b[39mvalues)  \u001b[39m# Insert the ID column at the start\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:946\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    944\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    945\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[0;32m--> 946\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[1;32m    948\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    949\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:641\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     force_all_finite \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    642\u001b[0m     X,\n\u001b[1;32m    643\u001b[0m     dtype\u001b[39m=\u001b[39;49mDTYPE,\n\u001b[1;32m    644\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    645\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    646\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    647\u001b[0m )\n\u001b[1;32m    648\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[1;32m    649\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    610\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n"
     ]
    }
   ],
   "source": [
    "train_B_X = train_B.drop(columns=['subjectivePoverty_rating'])\n",
    "P_RF = predict_ratings_RF(model_rf, train_B_X)\n",
    "P_RF.to_csv(os.path.join(\"../data/train_B_preds/train_B_preds_rf.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
